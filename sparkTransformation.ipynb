{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46580942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c875b0",
   "metadata": {},
   "source": [
    "### 5.1 map (NARROW) — change each item \n",
    "#### ● What: One input → one output, element by element. \n",
    "#### ● Why: Clean, fast; stays local; great for simple conversions. \n",
    "#### ● Tiny example (RDD): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70eb0d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 12]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"Spark fast\", \"PySpark easy\"]) \n",
    "lengths = lines.map(lambda s: len(s))   # lazy \n",
    "lengths.take(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697328f0",
   "metadata": {},
   "source": [
    "### 5.2 flatMap (NARROW) — expand/split each item \n",
    "#### ● What: One input → many outputs (e.g., split sentences into words). \n",
    "#### ● Why: Perfect for tokenizing text. \n",
    "#### ● Tiny example (RDD): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4d7cd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'fast', 'PySpark', 'easy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda s: s.split()) \n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73603802",
   "metadata": {},
   "source": [
    "### 5.3 filter (NARROW) — keep rows that match \n",
    "#### ● What: Keep only items that pass a condition.\n",
    "#### ● Why: Remove noise early (before wide steps). \n",
    "#### ● Tiny example (RDD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a531bc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'PySpark']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_words = words.filter(lambda w: len(w) >= 5) \n",
    "long_words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53a02a",
   "metadata": {},
   "source": [
    "### 5.4 withColumn / select (DataFrame, NARROW)\n",
    "#### ● What: Add/change columns; choose columns. \n",
    "#### ● Why: Column-wise transforms are local (narrow). \n",
    "#### ● Tiny example (DF): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ab55b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n",
      "|name|qty|price|\n",
      "+----+---+-----+\n",
      "|Ravi|  2|100.0|\n",
      "|Sita|  4|200.0|\n",
      "+----+---+-----+\n",
      "\n",
      "+----+------+\n",
      "|name|amount|\n",
      "+----+------+\n",
      "|Ravi| 200.0|\n",
      "|Sita| 800.0|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Ravi\", 2, 100.0),\n",
    "        (\"Sita\", 4, 200.0)\n",
    "    ],\n",
    "    [\"name\", \"qty\", \"price\"]\n",
    ")\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    " \n",
    "df2 = df.withColumn(\"amount\", \n",
    "F.col(\"qty\")*F.col(\"price\")).select(\"name\",\"amount\") \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17919c",
   "metadata": {},
   "source": [
    "### 5.5 groupByKey vs reduceByKey (RDD, WIDE) \n",
    "#### ● What: Both group by key, but reduceByKey is more efficient for sums/counts. \n",
    "#### ● Why: reduceByKey combines values before shuffling (less data moved). \n",
    "#### ● Tiny example (RDD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59da1816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = sc.parallelize([(\"a\",1),(\"a\",1),(\"b\",1),(\"a\",1),(\"b\",1)])\n",
    "# groupByKey -> heavier shuffle\n",
    "g = pairs.groupByKey()\n",
    "sum_g = g.mapValues(lambda it: sum(it))\n",
    "sum_g.collect()   # ACTION -> [('a', 3), ('b', 2)]\n",
    "# reduceByKey -> preferred\n",
    "sum_r = pairs.reduceByKey(lambda a,b: a+b)\n",
    "sum_r.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f5892",
   "metadata": {},
   "source": [
    "### 5.6 join (WIDE) \n",
    "#### ● What: Combine two datasets by key. \n",
    "#### ● Why: Very common; can be expensive. Use broadcast for small tables (in DF API). \n",
    "#### ● Tiny example (DF with broadcast):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61f3f6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|region|\n",
      "+---+----+------+\n",
      "|  1|Ravi|  East|\n",
      "|  2|Sita|  West|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left  = spark.createDataFrame([(1,\"Ravi\"),(2,\"Sita\")], \n",
    "[\"id\",\"name\"]) \n",
    "right = spark.createDataFrame([(1,\"East\"),(2,\"West\")], \n",
    "[\"id\",\"region\"]) \n",
    "from pyspark.sql.functions import broadcast \n",
    "joined = left.join(broadcast(right), \"id\", \"left\") \n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c687f95",
   "metadata": {},
   "source": [
    "### 5.7 orderBy/sort (WIDE) \n",
    "#### ● What: Sort rows. \n",
    "#### ● Why: Forces shuffle + sort; use only when needed. \n",
    "#### ● Tiny example (DF): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20fbf24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+------+\n",
      "|name|qty|price|amount|\n",
      "+----+---+-----+------+\n",
      "|Sita|  4|200.0| 800.0|\n",
      "|Ravi|  2|100.0| 200.0|\n",
      "+----+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"amount\", F.col(\"qty\") * F.col(\"price\"))\n",
    "df2.orderBy(F.desc(\"amount\")).show()\n",
    "\n",
    "\n",
    "# df.orderBy(F.desc(\"amount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046dea2",
   "metadata": {},
   "source": [
    "### 5.8 distinct/dropDuplicates (WIDE) \n",
    "#### ● What: Remove duplicates. \n",
    "#### ● Why: Requires shuffling to find duplicates across partitions. \n",
    "#### ● Tiny example (DF): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff612dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Ravi|\n",
      "|Sita|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7c20467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[137] at RDD at PythonRDD.scala:56"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel \n",
    "pairs = sc.parallelize([(\"a\",1),(\"a\",1),(\"b\",1)]) \n",
    "hot = pairs.reduceByKey(lambda a,b: a+b)         \n",
    "hot.persist(StorageLevel.MEMORY_AND_DISK)        \n",
    "hot.count()\n",
    "print(hot.take(5))                               \n",
    "hot.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1d126f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|word|cnt|\n",
      "+----+---+\n",
      "|   c|  3|\n",
      "|   b|  2|\n",
      "|   a|  3|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, cnt: bigint]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "words_df = spark.createDataFrame([(\"a a b\",), (\"b c c\",), (\"a c\",)], [\"line\"]).select(F.explode(F.split(\"line\", r\"\\s+\")).alias(\"word\")) \n",
    "counts_df = words_df.groupBy(\"word\").agg(F.count(\"*\").alias(\"cnt\")) \n",
    "hot_df = counts_df.cache()\n",
    "hot_df.count()\n",
    "hot_df.show(5)\n",
    "hot_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fad2b389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records per partition: [0, 0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "pairs = sc.parallelize([(\"a\",1),(\"a\",1),(\"b\",2),(\"c\",3)]) \n",
    "reduced = pairs.reduceByKey(lambda a,b: a+b)   # WIDE \n",
    "reduced_p = reduced.partitionBy(4)   # WIDE\n",
    "# (one-time), 4 partitions \n",
    "# Inspect partition sizes \n",
    "sizes = reduced_p.mapPartitions(lambda it: [sum(1 for _ in \n",
    "it)]).collect() \n",
    "print(\"records per partition:\", sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "909ffe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', 3), ('a', 3), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "text   = sc.parallelize([\"a a b\", \"b c c\", \"a c\"]) \n",
    "words  = text.flatMap(lambda s: s.split())            # NARROW\n",
    "pairs  = words.map(lambda w: (w,1))                   # NARROW\n",
    "counts = pairs.reduceByKey(lambda a,b: a+b)           # WIDE\n",
    "# (shuffle) \n",
    "print(counts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e52b95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|word|cnt|\n",
      "+----+---+\n",
      "|   a|  3|\n",
      "|   c|  3|\n",
      "|   b|  2|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "df  = spark.createDataFrame([(\"a a b\",), (\"b c c\",), (\"a c\",)], [\"line\"]) \n",
    "out = (df .select(F.explode(F.split(\"line\", r\"\\s+\")).alias(\"word\")).groupBy(\"word\").agg(F.count(\"*\").alias(\"cnt\")).orderBy(F.desc(\"cnt\")))\n",
    "out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfe87cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD counts: [('c', 3), ('a', 3), ('b', 2)]\n",
      "Top after cache: [('c', 3), ('a', 3), ('b', 2)]\n",
      "Records per partition: [0, 0, 2, 1]\n",
      "+----+---+\n",
      "|word|cnt|\n",
      "+----+---+\n",
      "|   a|  3|\n",
      "|   c|  3|\n",
      "|   b|  2|\n",
      "+----+---+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['cnt DESC NULLS LAST], true\n",
      "+- Aggregate [word#279], [word#279, count(1) AS cnt#280L]\n",
      "   +- Project [word#279]\n",
      "      +- Generate explode(split(line#277, \\s+, -1)), false, [word#279]\n",
      "         +- LogicalRDD [line#277], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "word: string, cnt: bigint\n",
      "Sort [cnt#280L DESC NULLS LAST], true\n",
      "+- Aggregate [word#279], [word#279, count(1) AS cnt#280L]\n",
      "   +- Project [word#279]\n",
      "      +- Generate explode(split(line#277, \\s+, -1)), false, [word#279]\n",
      "         +- LogicalRDD [line#277], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [cnt#280L DESC NULLS LAST], true\n",
      "+- Aggregate [word#279], [word#279, count(1) AS cnt#280L]\n",
      "   +- Generate explode(split(line#277, \\s+, -1)), [0], false, [word#279]\n",
      "      +- LogicalRDD [line#277], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [cnt#280L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(cnt#280L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=527]\n",
      "      +- HashAggregate(keys=[word#279], functions=[count(1)], output=[word#279, cnt#280L])\n",
      "         +- Exchange hashpartitioning(word#279, 200), ENSURE_REQUIREMENTS, [plan_id=524]\n",
      "            +- HashAggregate(keys=[word#279], functions=[partial_count(1)], output=[word#279, count#286L])\n",
      "               +- Generate explode(split(line#277, \\s+, -1)), false, [word#279]\n",
      "                  +- Scan ExistingRDD[line#277]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Setup --- \n",
    "from pyspark.sql import SparkSession, functions as F \n",
    "from pyspark import StorageLevel \n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Beginner-Chapter\").getOrCreate() \n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# --- RDD pipeline: narrow -> wide -> action --- \n",
    "text   = sc.parallelize([\"a a b\", \"b c c\", \"a c\"]) \n",
    "words  = text.flatMap(lambda s: s.split())\n",
    "# narrow (lazy) \n",
    "pairs  = words.map(lambda w: (w,1))\n",
    "# narrow (lazy) \n",
    "counts = pairs.reduceByKey(lambda a,b: a+b)\n",
    "# wide (shuffle, lazy) \n",
    "print(\"RDD counts:\", counts.collect())\n",
    "# ACTION\n",
    "\n",
    "\n",
    "# --- Cache / persist (RDD) ---\n",
    "hot = pairs.reduceByKey(lambda a,b: a+b)\n",
    "# wide \n",
    "hot.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "hot.count()\n",
    "\n",
    "# ACTION (materialize)\n",
    "print(\"Top after cache:\", hot.take(10))\n",
    "hot.unpersist()\n",
    "\n",
    "\n",
    "# --- Partition by key (RDD) ---\n",
    "counts_p = counts.partitionBy(4)\n",
    "# one-time reshuffle \n",
    "sizes = counts_p.mapPartitions(lambda it: [sum(1 for _ in \n",
    "it)]).collect()\n",
    "print(\"Records per partition:\", sizes)\n",
    "# ACTION\n",
    "\n",
    "# --- DataFrame version (same task, optimized) --- \n",
    "df  = spark.createDataFrame([(\"a a b\",), (\"b c c\",), (\"a c\",)], [\"line\"]) \n",
    "out = (df.select(F.explode(F.split(\"line\", r\"\\s+\")).alias(\"word\"))  # narrow \n",
    "        .groupBy(\"word\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "# wide \n",
    ".orderBy(F.desc(\"cnt\")))\n",
    "# wide \n",
    "out.show()\n",
    "# ACTION \n",
    "\n",
    "\n",
    "# Peek at physical plan (how Spark will execute) \n",
    "out.explain(\"extended\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
